{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBRFC_6321.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlPUm9hsDvaVbDBkxTDYyM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9K_IUcr3yna"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
        "from xgboost import XGBRFClassifier\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhVy3z7V36b4"
      },
      "source": [
        "# Examination of Premier League results from FiveThirtyEight's SPI dataset.\n",
        "# We will compare the projected vs. actual results and see if they match, and then train an XGB Random Forest Classifier to predict the \"match\" (or lack thereof).\n",
        "# Finally, we will run this for 100 iterations and calculate the minimum, mean, and maximum accuracy values based on changing random_state values.\n",
        "df = pd.read_csv(\"https://projects.fivethirtyeight.com/soccer-api/club/spi_matches.csv\").dropna()\n",
        "df = df[df.league==\"Barclays Premier League\"].reset_index(drop=True)"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-XvZvbw4L0U"
      },
      "source": [
        "# Comparing the projected home margins vs. the actual home margins.\n",
        "df['proj_h_margin'] = df['proj_score1'] - df['proj_score2']\n",
        "df['act_h_margin'] = df['score1'] - df['score2']"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDJjEj_U4d5a"
      },
      "source": [
        "projs = []\n",
        "acts = []"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE_Gh80C4bE-"
      },
      "source": [
        "# The projected result is derived from the projected home margin.\n",
        "for x in df['proj_h_margin']:\n",
        "  if x > 0:\n",
        "    projs.append(\"PROJECTED HOME\")\n",
        "  elif x < 0:\n",
        "    projs.append(\"PROJECTED AWAY\")\n",
        "  else:\n",
        "    projs.append(\"PROJECTED DRAW\")"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_LnFBGq4IWF"
      },
      "source": [
        "# Likewise, the actual result is derived from the actual home margin.\n",
        "for y in df['act_h_margin']:\n",
        "  if y > 0:\n",
        "    acts.append(\"HOME\")\n",
        "  elif y < 0:\n",
        "    acts.append(\"AWAY\")\n",
        "  else:\n",
        "    acts.append(\"DRAW\")"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y9NrX9i5aUS"
      },
      "source": [
        "# We don't need these columns for the construction of the model.\n",
        "df = df.drop(columns=['proj_h_margin', 'act_h_margin'])"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUe0JPG75gi5"
      },
      "source": [
        "df['proj_result'] = projs\n",
        "df['result'] = acts"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXTKg50n6VWy"
      },
      "source": [
        "matches = []"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl1kdGCl5lnm"
      },
      "source": [
        "# This is where the rubber hits the road, or where we compare the projected results vs. the actual results.\n",
        "# If they match, it's \"MATCH.\" If they don't, it's \"NO MATCH.\"\n",
        "for z in range(len(df)):\n",
        "  p = df['proj_result'][z]\n",
        "  a = df['result'][z]\n",
        "  if p == \"PROJECTED HOME\" and a == \"HOME\":\n",
        "    matches.append(\"MATCH\")\n",
        "  elif p == \"PROJECTED HOME\" and a == \"AWAY\":\n",
        "    matches.append(\"NO MATCH\")\n",
        "  elif p == \"PROJECTED HOME\" and a == \"DRAW\":\n",
        "    matches.append(\"NO MATCH\")\n",
        "  elif p == \"PROJECTED AWAY\" and a == \"AWAY\":\n",
        "    matches.append(\"MATCH\")\n",
        "  elif p == \"PROJECTED AWAY\" and a == \"HOME\":\n",
        "    matches.append(\"NO MATCH\")\n",
        "  elif p == \"PROJECTED AWAY\" and a == \"DRAW\":\n",
        "    matches.append(\"NO MATCH\")\n",
        "  elif p == \"PROJECTED DRAW\" and a == \"HOME\":\n",
        "    matches.append(\"NO MATCH\")\n",
        "  elif p == \"PROJECTED DRAW\" and a == \"AWAY\":\n",
        "    matches.append(\"NO MATCH\")\n",
        "  else:\n",
        "    matches.append(\"MATCH\")"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3eVIPu04Cei"
      },
      "source": [
        "df['match'] = matches"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNY_Gp3_6tqg"
      },
      "source": [
        "# Defining the predictor and targets.\n",
        "X_final = df[['proj_score1','proj_score2','score1','score2']]\n",
        "y_final = df['match']"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IA0he2Q80Vv"
      },
      "source": [
        "a_score = []"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8-iDU7M69lP"
      },
      "source": [
        "def model():\n",
        "  # This is the heavy lifting part of the experiment, where we generate 100 rounds of predictions and the random_state increases by 1 each time.\n",
        "  # After each iteration, the accuracy value is appended to a list where we will examine the min, mean, and max values.\n",
        "  count = 0\n",
        "  while count < 100:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=count)\n",
        "    model = XGBRFClassifier(max_depth=6, objective='binary:logistic', n_estimators=400).fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    a_score.append(accuracy)\n",
        "    count += 1"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Rzkf8Y8751"
      },
      "source": [
        "model = model()"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aYtwSXy9Zcy",
        "outputId": "e9aa37ed-0822-4373-dde7-6f154bb9d412"
      },
      "source": [
        "print(\"Accuracy Values: Minimum: {} | Mean: {} | Max: {}\".format(np.round(np.min(a_score),3), np.round(np.mean(a_score),3), np.round(np.max(a_score),3)))"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Values: Minimum: 0.933 | Mean: 0.966 | Max: 0.987\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}