{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SPI_XGBRF_52821.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUU/CXsjZ9P8BTR+VPeOg2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WiWoYME-AZA"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uki2YenK-Dcf"
      },
      "source": [
        "# In looking at FiveThirtyEight's soccer statistics, we are eager to compare results (HOME WIN, AWAY WIN, and DRAW)\n",
        "# to the pre-game probabilities assigned to each team and determine whether or not the favorite emerged victorious.\n",
        "df = pd.read_csv(\"https://projects.fivethirtyeight.com/soccer-api/club/spi_matches.csv\").dropna()"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njh_eXom-zHa"
      },
      "source": [
        "# First, we generate the home margin to calculate the net number of goals the home team scored vs. the away side.\n",
        "df['h_margin'] = df['score1'] - df['score2']"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N3xmfMG_2Ng"
      },
      "source": [
        "# Our empty list which will be populated with results.\n",
        "results = []"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWD_1f9-_4Kw"
      },
      "source": [
        "# If the home margin is greater than 0, it's a home win.\n",
        "# If it equals 0, it's a draw.\n",
        "# If it's less than 0, it's an away win.\n",
        "for x in df['h_margin']:\n",
        "  if x > 0:\n",
        "    results.append(\"HOME WIN\")\n",
        "  elif x == 0:\n",
        "    results.append(\"DRAW\")\n",
        "  else:\n",
        "    results.append(\"AWAY WIN\")"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amMa5NrU__o0"
      },
      "source": [
        "# Add results to the dataframe.\n",
        "df['result'] = results"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3VNZRXIACDE"
      },
      "source": [
        "# Now we want to examine the home probability margin. Excluding the probability of a draw, which team\n",
        "# (home or away) has a greater probability of winning?\n",
        "df['h_prob_margin'] = df['prob1'] - df['prob2']\n",
        "prob_margins = []"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__wZx3tDAdDr"
      },
      "source": [
        "# If the home team has a margin of greater than or equal to 0 (unlikely == 0 since the range of values is 0-1),\n",
        "# then the home team is the favorite. The away team, the inverse.\n",
        "for y in df['h_prob_margin']:\n",
        "  if y >= 0:\n",
        "    prob_margins.append(\"HOME FAVORITE\")\n",
        "  else:\n",
        "    prob_margins.append(\"AWAY FAVORITE\")"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skNU0XiTAs1L"
      },
      "source": [
        "# Add the win probability buckets to the dataframe.\n",
        "df['prob_bucket'] = prob_margins"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfCDWqQ3DXiw"
      },
      "source": [
        "# No need for the h_margin and h_prob_margin columns anymore.\n",
        "df = df.drop(columns=['h_margin', 'h_prob_margin'])"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUD22gm9A3eX"
      },
      "source": [
        "# We want to judge whether the pre-match probabilities (in terms of who is favored to win) is equal to the outcome.\n",
        "comparison_df = df[['result','prob_bucket']]\n",
        "comparisons = []"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54dUf6r8BjCx"
      },
      "source": [
        "# Generating the range for the length of the comparison dataframe, which is also the length of the main df.\n",
        "ran = range(0, len(comparison_df), 1)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80Us-vIzEb1A"
      },
      "source": [
        "# If home was favored and won, it's \"EXPECTED HOME\".\n",
        "# If away was favored and won, \"EXPECTED AWAY.\"\n",
        "# If away was favored and lost, it's \"UNEXPECTED HOME\".\n",
        "# If home was favored and lost, it's \"UNEXPECTED AWAY\"\n",
        "# Otherwise, it's \"no decisive outcome,\" which means a draw.\n",
        "for z in ran:\n",
        "  z = comparison_df.iloc[z,:]\n",
        "  if (z[0]==\"HOME WIN\") & (z[1]==\"HOME FAVORITE\"):\n",
        "    comparisons.append(\"EXPECTED HOME\")\n",
        "  elif (z[0]==\"AWAY WIN\") & (z[1]==\"AWAY FAVORITE\"):\n",
        "    comparisons.append(\"EXPECTED AWAY\")\n",
        "  elif (z[0]==\"AWAY WIN\") & (z[1]==\"HOME FAVORITE\"):\n",
        "    comparisons.append(\"UNEXPECTED AWAY\")\n",
        "  elif (z[0]==\"HOME WIN\") & (z[1]==\"AWAY FAVORITE\"):\n",
        "    comparisons.append(\"UNEXPECTED HOME\")\n",
        "  else:\n",
        "    comparisons.append(\"NO DECISIVE OUTCOME\")"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek763J0PH6_X"
      },
      "source": [
        "# Adding the comparisons to the dataframe. This will be our target variable, so probably not necessary, but I prefer it for consistency.\n",
        "df['result_type'] = comparisons"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJFRbxomVeKl"
      },
      "source": [
        "# Predictor and target definition. We will use both the scores and probabilitiesof home and away.\n",
        "X = df[['score1','score2','prob1','prob2']]\n",
        "y = df['result_type']"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbY2vA1hVlGc"
      },
      "source": [
        "# Since the amount of data is pretty extensive, 0.2 is a fine test size.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_vHK8-lVwdY"
      },
      "source": [
        "# The normal XGBRF depth is 3, but we'll use 6.\n",
        "model = XGBRFClassifier(objective='multiclass:softprob', max_depth=6).fit(X_train, y_train)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJvPxrhaWAvY"
      },
      "source": [
        "# Generating predictions based on the test predictors.\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYpN00WMXTli"
      },
      "source": [
        "# Accuracy to the 1000th should be suitable. Since we're generating an accuracy percentage to close out the experiment, we will multiply the rounded result by 100.\n",
        "accuracy = np.round(accuracy_score(y_test, predictions) * 100, 3)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZHkpoAGZbwB",
        "outputId": "a412c363-a59d-4a40-c002-0225bd13b71b"
      },
      "source": [
        "print(\"Test Accuracy: {}%\".format(accuracy))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 99.451%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}