{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBC_6721_master.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6LXstT0HZyhnpvfAKJwyt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sl2Y-155QS9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 359,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZJvckMI5Wzs"
      },
      "source": [
        "# Soccer is a game of fine margins. As such, even when one teams outscores the other, there may be variation in other non-scoring metrics, such as:\n",
        "  # SPI (Soccer Power Index) ratings\n",
        "  # xG (Expected Goal) tallies\n",
        "  # Win probability %\n",
        "  # Adjusted xG\n",
        "  # Non-Shot xG\n",
        "# As such, we will run through the FiveThirtyEight SPI dataset, examining Barclays Premier League matches between 2018-19 and 2020-21.\n",
        "# If all indicators are in favor of one team, we will label them as \"UNANIMOUS\", meaning that the performance indicators were unanimous in favor of one team.\n",
        "# If the indicatorse are mixed between favoring one side or the other, it will be \"MIXED.\"\n",
        "df = pd.read_csv(\"https://projects.fivethirtyeight.com/soccer-api/club/spi_matches.csv\").dropna()\n",
        "df = df[(df.league==\"Barclays Premier League\") & (df.season >= 2018)].reset_index(drop=True)"
      ],
      "execution_count": 361,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4imrmGA6OLL"
      },
      "source": [
        "# First, we need to generate comparisons between the performance metrics of the home and away team.\n",
        "# Usually margins like these would be generated as absolute values, but since we want to know how the teams stack up relative to one another,\n",
        "# we need negative values to clearly differentiate the teams, so we're just subtracing the away performance from the home performance.\n",
        "df['h_result'] = df['score1'] - df['score2']\n",
        "df['h_spi'] = df['spi1'] - df['spi2']\n",
        "df['h_xg'] = df['xg1'] - df['xg2']\n",
        "df['h_prob'] = df['prob1'] - df['prob2']\n",
        "df['h_adj'] = df['adj_score1'] - df['adj_score2']\n",
        "df['h_nsxg'] = df['nsxg1'] - df['nsxg2']"
      ],
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aPKvCBv7JVh"
      },
      "source": [
        "results = []\n",
        "spis = []\n",
        "xgs = []\n",
        "probs = []\n",
        "adjs = []\n",
        "nsxgs = []"
      ],
      "execution_count": 363,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNH_0djy6Uit"
      },
      "source": [
        "# This function does a lot of the heavy lifting.\n",
        "# If a performance metric is greater than 0 (thus, in favor of the home team), it's \"HOME.\"\n",
        "# Else, if a performance metric is less than 0 (thus, in favor of the away team), it's \"AWAY.\"\n",
        "# Else, it's a \"DRAW\" (unlikely with certain metrics like xG, which go to the hundredths decimal point, but not important since we are comparing all six).\n",
        "def convert(array, list_to_return):\n",
        "  array = df[array]\n",
        "  for x in range(len(array)):\n",
        "    x = array[x]\n",
        "    if x > 0:\n",
        "      list_to_return.append(\"HOME\")\n",
        "    elif x < 0:\n",
        "      list_to_return.append(\"AWAY\")\n",
        "    else:\n",
        "      list_to_return.append(\"DRAW\")"
      ],
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA6TsDY28nzm"
      },
      "source": [
        "result = convert(\"h_result\", results)\n",
        "spi = convert(\"h_spi\", spis)\n",
        "xg = convert(\"h_xg\", xgs)\n",
        "prob = convert(\"h_prob\", probs)\n",
        "adj = convert(\"h_adj\", adjs)\n",
        "nsxg = convert(\"h_nsxg\", nsxgs)"
      ],
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b4r7JYYBJet"
      },
      "source": [
        "# Inserting our newly-generated lists into the dataframe at the expense of the original calculations.\n",
        "df = df.drop(columns=['h_result','h_spi','h_xg','h_prob','h_adj','h_nsxg'])\n",
        "df['result'] = results\n",
        "df['spi'] = spis\n",
        "df['xg'] = xgs\n",
        "df['prob'] = probs\n",
        "df['adj'] = adjs\n",
        "df['nsxg'] = xgs"
      ],
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYAyDtG7CRBR"
      },
      "source": [
        "y_list = []\n",
        "master_result = []"
      ],
      "execution_count": 367,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB8i8a3PAzNf"
      },
      "source": [
        "# We create a list for each row of the dataset, containing only our newly-generated metrics.\n",
        "for y in range(len(df)):\n",
        "  y1 = df['result'][y]\n",
        "  y2 = df['spi'][y]\n",
        "  y3 = df['xg'][y]\n",
        "  y4 = df['prob'][y]\n",
        "  y5 = df['adj'][y]\n",
        "  y6 = df['nsxg'][y]\n",
        "  y_list.append(list([y1, y2, y3, y4, y5, y6]))"
      ],
      "execution_count": 368,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-mWeTBzB0P-"
      },
      "source": [
        "# If all the elements in the row list are the same, it's \"UNANIMOUS.\" Otherwise, \"MIXED.\"\n",
        "for z in range(len(y_list)):\n",
        "  z = y_list[z]\n",
        "  a = z.count(z[0])==len(z)\n",
        "  if a == True:\n",
        "    master_result.append(\"UNANIMOUS\")\n",
        "  else:\n",
        "    master_result.append(\"MIXED\")"
      ],
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITwJslq7Gq2m"
      },
      "source": [
        "df['master'] = master_result"
      ],
      "execution_count": 370,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAXaeeI6Cel1"
      },
      "source": [
        "# Splitting the relevant metrics into predictors or targets.\n",
        "X_final = df[['score1','score2','spi1','spi2','xg1','xg2','prob1','prob2','adj_score1','adj_score2','nsxg1','nsxg2']]\n",
        "y_final = df['master']"
      ],
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEK6-7DfG7hu"
      },
      "source": [
        "accuracy_values = []"
      ],
      "execution_count": 374,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2hR1QObEIeq"
      },
      "source": [
        "# The model will run for 100 rounds, with the random_state increasing by 1 each time so our results are reproducible.\n",
        "def model():\n",
        "  global count\n",
        "  count = 0\n",
        "  while count < 100:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.25, random_state=count)\n",
        "    model = XGBClassifier(n_estimators=400, eval_metric='mlogloss')\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    accuracy_values.append(accuracy)\n",
        "    count += 1"
      ],
      "execution_count": 375,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rngbYe6jEsR6"
      },
      "source": [
        "model = model()"
      ],
      "execution_count": 376,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ilC9RXyHN2E"
      },
      "source": [
        "# Quick function to convert the accuracy values to integers (to compute percentages).\n",
        "def round_it(num):\n",
        "  num = int(num * 100)\n",
        "  return num"
      ],
      "execution_count": 395,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV5HKdkcHTCH"
      },
      "source": [
        "acc_min = round_it(np.min(accuracy_values))\n",
        "acc_med = round_it(np.median(accuracy_values))\n",
        "acc_mean = round_it(np.mean(accuracy_values))\n",
        "acc_max = round_it(np.max(accuracy_values))"
      ],
      "execution_count": 396,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXV2mXzcHiI4",
        "outputId": "2d851b1b-ab10-41bd-c034-b1bf88c7baea"
      },
      "source": [
        "print(\"Accuracy Measures over {} Rounds:\".format(count))\n",
        "print(\"Minimum: {}% | Median: {}% | Mean: {}% | Maximum: {}%\".format(acc_min, acc_med, acc_mean, acc_max))"
      ],
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Measures over 100 Rounds:\n",
            "Minimum: 90% | Median: 93% | Mean: 93% | Maximum: 97%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}